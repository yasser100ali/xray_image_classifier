{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9328577,
          "sourceType": "datasetVersion",
          "datasetId": 5651871
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Goal of this notebook is to take our x-ray png files and build a convolutional neural network to predict which images are part of 'findings' and which are part of 'no-findings'\n",
        "\n",
        "Procedure and Outline\n",
        "\n",
        "Loading the data\n",
        "1. Use ImageDataGenerator to create augmentations of the data\n",
        "2. Split into training and validation subset\n",
        "\n",
        "Training The Model\n",
        "1. Load the pretrained models\n",
        "2. Set trainable to False\n",
        "3. Create a custom layer on top of this model -> Convolutional Layer, MaxPooling2D, Dense(128), Dense(1).\n",
        "4. * Experiment with dropout to see if it helps\n",
        "5. Compile the model -> Set the learning rate (tf.keras.?.adam), loss='binary_crossentropy' since it is a binary classification problem, metric=['accuracy'] (or MatthewCorrelationCoefficient see step 1 of Testing)\n",
        "6. Create history = model.fit(train=(X_train, y_train), validation=(X_val, y_val), epochs=5, verbose=1, batch_size=32)\n",
        "\n",
        "Testing\n",
        "1. Create a function to get the metric (using Matthew Correlation Coefficient) -> may have to do this prior to compiling the model and set this function as the metric. Look more into this, test both.\n",
        "2. Create an array, y_test = model.predict(X_test)\n",
        "3. Submit and see score\n",
        "\n",
        "Assessment\n",
        "Try to find weaknesses and see where you can Improve\n",
        "\"\"\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-06T01:38:54.117403Z",
          "iopub.execute_input": "2024-09-06T01:38:54.117798Z",
          "iopub.status.idle": "2024-09-06T01:38:54.125148Z",
          "shell.execute_reply.started": "2024-09-06T01:38:54.117764Z",
          "shell.execute_reply": "2024-09-06T01:38:54.124263Z"
        },
        "trusted": true,
        "id": "2J1_4p9SvYub",
        "outputId": "da637d20-d120-4bd4-fea1-f0f412b8358c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 91,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"\\nGoal of this notebook is to take our x-ray png files and build a convolutional neural network to predict which images are part of 'findings' and which are part of 'no-findings'\\n\\nProcedure and Outline\\n\\nLoading the data\\n1. Use ImageDataGenerator to create augmentations of the data \\n2. Split into training and validation subset\\n\\nTraining The Model\\n1. Load the pretrained models\\n2. Set trainable to False\\n3. Create a custom layer on top of this model -> Convolutional Layer, MaxPooling2D, Dense(128), Dense(1). \\n4. * Experiment with dropout to see if it helps\\n5. Compile the model -> Set the learning rate (tf.keras.?.adam), loss='binary_crossentropy' since it is a binary classification problem, metric=['accuracy'] (or MatthewCorrelationCoefficient see step 1 of Testing)\\n6. Create history = model.fit(train=(X_train, y_train), validation=(X_val, y_val), epochs=5, verbose=1, batch_size=32)\\n\\nTesting\\n1. Create a function to get the metric (using Matthew Correlation Coefficient) -> may have to do this prior to compiling the model and set this function as the metric. Look more into this, test both.\\n2. Create an array, y_test = model.predict(X_test)\\n3. Submit and see score\\n\\nAssessment\\nTry to find weaknesses and see where you can Improve\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reNRWmSqCEhg",
        "outputId": "25581d79-6953-430b-ed2d-6b641a98074e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# image processing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# machine learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, BatchNormalization, Concatenate, Flatten\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:33:43.151781Z",
          "iopub.execute_input": "2024-09-06T03:33:43.152268Z",
          "iopub.status.idle": "2024-09-06T03:33:58.065921Z",
          "shell.execute_reply.started": "2024-09-06T03:33:43.152220Z",
          "shell.execute_reply": "2024-09-06T03:33:58.064682Z"
        },
        "trusted": true,
        "id": "dqED6EqavYud"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = '/content/drive/MyDrive/xray_project/Train_PNG'\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale = 1.0/255.00,\n",
        "    rotation_range = 10,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    zoom_range = 0.1,\n",
        "    validation_split = 0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory = train_file_path,\n",
        "    target_size = (512, 512),\n",
        "    color_mode = 'rgb',\n",
        "    class_mode = 'binary',\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    directory = train_file_path,\n",
        "    target_size = (512, 512),\n",
        "    color_mode = 'rgb',\n",
        "    class_mode = 'binary',\n",
        "    batch_size = 32,\n",
        "    shuffle = False,\n",
        "    subset = 'validation'\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-05T21:42:24.561327Z",
          "iopub.execute_input": "2024-09-05T21:42:24.562331Z",
          "iopub.status.idle": "2024-09-05T21:42:29.207136Z",
          "shell.execute_reply.started": "2024-09-05T21:42:24.562283Z",
          "shell.execute_reply": "2024-09-05T21:42:29.206365Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwqDRU-PvYue",
        "outputId": "7effc3aa-3773-4168-ebbb-2cf002944bb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8001 images belonging to 2 classes.\n",
            "Found 1999 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(512, 512, 3))\n",
        "\n",
        "mobilenet_base = MobileNetV2(weights='imagenet', input_shape=(512, 512, 3), include_top=False)\n",
        "densenet_base = DenseNet169(weights='imagenet', input_shape=(512, 512, 3), include_top=False)\n",
        "\n",
        "# set the layers not trainable to preserve weights of pretrained model\n",
        "for layer in mobilenet_base.layers:\n",
        "    layer.trainable=False\n",
        "for layer in densenet_base.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "model_mobilenet = mobilenet_base(input_layer)\n",
        "model_mobilenet = GlobalAveragePooling2D()(model_mobilenet)\n",
        "output_mobilenet = Flatten()(model_mobilenet)\n",
        "\n",
        "model_densenet = densenet_base(input_layer)\n",
        "model_densenet = GlobalAveragePooling2D()(model_densenet)\n",
        "output_densenet = Flatten()(model_densenet)\n",
        "\n",
        "merged = Concatenate()([output_mobilenet, output_densenet])\n",
        "\n",
        "x = BatchNormalization()(merged)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T01:39:28.489797Z",
          "iopub.execute_input": "2024-09-06T01:39:28.490562Z",
          "iopub.status.idle": "2024-09-06T01:39:33.849973Z",
          "shell.execute_reply.started": "2024-09-06T01:39:28.490507Z",
          "shell.execute_reply": "2024-09-06T01:39:33.848982Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXEygikvvYuf",
        "outputId": "620b44b9-5dc2-4f6d-d59c-007a08a7602a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8a6a3cdfc89b>:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet_base = MobileNetV2(weights='imagenet', input_shape=(512, 512, 3), include_top=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in mobilenet_base.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "for layer in densenet_base.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    factor=0.5,\n",
        "    min_lr=0.0001\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='model_checkpoint.weights.h5',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T01:39:51.191189Z",
          "iopub.execute_input": "2024-09-06T01:39:51.191606Z",
          "iopub.status.idle": "2024-09-06T01:39:51.206730Z",
          "shell.execute_reply.started": "2024-09-06T01:39:51.191562Z",
          "shell.execute_reply": "2024-09-06T01:39:51.205851Z"
        },
        "trusted": true,
        "id": "aVIEFPEUvYuf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_callback, lr_reducer, early_stopping] # run again to see effects of lr_reducer and early_stopping\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T01:40:05.867572Z",
          "iopub.execute_input": "2024-09-06T01:40:05.867967Z",
          "iopub.status.idle": "2024-09-06T03:14:07.804894Z",
          "shell.execute_reply.started": "2024-09-06T01:40:05.867929Z",
          "shell.execute_reply": "2024-09-06T03:14:07.804012Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0So9eJLvYug",
        "outputId": "2cc63f07-8324-404e-8c97-14634c798de7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.6413 - loss: 0.7383\n",
            "Epoch 1: val_accuracy improved from -inf to 0.72836, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1577s\u001b[0m 6s/step - accuracy: 0.6415 - loss: 0.7379 - val_accuracy: 0.7284 - val_loss: 0.5647 - learning_rate: 1.0000e-05\n",
            "Epoch 2/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7729 - loss: 0.5008\n",
            "Epoch 2: val_accuracy improved from 0.72836 to 0.79240, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 2s/step - accuracy: 0.7730 - loss: 0.5006 - val_accuracy: 0.7924 - val_loss: 0.4593 - learning_rate: 1.0000e-05\n",
            "Epoch 3/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8018 - loss: 0.4356\n",
            "Epoch 3: val_accuracy improved from 0.79240 to 0.84542, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 2s/step - accuracy: 0.8019 - loss: 0.4355 - val_accuracy: 0.8454 - val_loss: 0.3597 - learning_rate: 1.0000e-05\n",
            "Epoch 4/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8380 - loss: 0.3727\n",
            "Epoch 4: val_accuracy improved from 0.84542 to 0.87594, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.8380 - loss: 0.3727 - val_accuracy: 0.8759 - val_loss: 0.2995 - learning_rate: 1.0000e-05\n",
            "Epoch 5/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8492 - loss: 0.3474\n",
            "Epoch 5: val_accuracy improved from 0.87594 to 0.90195, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.8492 - loss: 0.3474 - val_accuracy: 0.9020 - val_loss: 0.2580 - learning_rate: 1.0000e-05\n",
            "Epoch 6/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8542 - loss: 0.3312\n",
            "Epoch 6: val_accuracy improved from 0.90195 to 0.90595, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 2s/step - accuracy: 0.8542 - loss: 0.3311 - val_accuracy: 0.9060 - val_loss: 0.2385 - learning_rate: 1.0000e-05\n",
            "Epoch 7/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8630 - loss: 0.3105\n",
            "Epoch 7: val_accuracy improved from 0.90595 to 0.90645, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 2s/step - accuracy: 0.8630 - loss: 0.3106 - val_accuracy: 0.9065 - val_loss: 0.2333 - learning_rate: 1.0000e-05\n",
            "Epoch 8/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8787 - loss: 0.2909\n",
            "Epoch 8: val_accuracy improved from 0.90645 to 0.91446, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.8787 - loss: 0.2909 - val_accuracy: 0.9145 - val_loss: 0.2226 - learning_rate: 1.0000e-05\n",
            "Epoch 9/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8760 - loss: 0.2942\n",
            "Epoch 9: val_accuracy improved from 0.91446 to 0.91746, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.8760 - loss: 0.2942 - val_accuracy: 0.9175 - val_loss: 0.2185 - learning_rate: 1.0000e-05\n",
            "Epoch 10/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8834 - loss: 0.2656\n",
            "Epoch 10: val_accuracy improved from 0.91746 to 0.92346, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 2s/step - accuracy: 0.8834 - loss: 0.2656 - val_accuracy: 0.9235 - val_loss: 0.2039 - learning_rate: 1.0000e-05\n",
            "Epoch 11/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8905 - loss: 0.2536\n",
            "Epoch 11: val_accuracy improved from 0.92346 to 0.92546, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.8905 - loss: 0.2536 - val_accuracy: 0.9255 - val_loss: 0.1958 - learning_rate: 1.0000e-05\n",
            "Epoch 12/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8917 - loss: 0.2567\n",
            "Epoch 12: val_accuracy improved from 0.92546 to 0.92596, saving model to model_checkpoint.weights.h5\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 2s/step - accuracy: 0.8917 - loss: 0.2567 - val_accuracy: 0.9260 - val_loss: 0.1974 - learning_rate: 1.0000e-05\n",
            "Epoch 13/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9082 - loss: 0.2222\n",
            "Epoch 13: val_accuracy did not improve from 0.92596\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 2s/step - accuracy: 0.9082 - loss: 0.2222 - val_accuracy: 0.9240 - val_loss: 0.1975 - learning_rate: 1.0000e-05\n",
            "Epoch 14/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9103 - loss: 0.2212\n",
            "Epoch 14: val_accuracy did not improve from 0.92596\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 2s/step - accuracy: 0.9103 - loss: 0.2213 - val_accuracy: 0.9240 - val_loss: 0.1896 - learning_rate: 1.0000e-05\n",
            "Epoch 15/15\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9087 - loss: 0.2210\n",
            "Epoch 15: val_accuracy did not improve from 0.92596\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 2s/step - accuracy: 0.9087 - loss: 0.2210 - val_accuracy: 0.9245 - val_loss: 0.1919 - learning_rate: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/drive/MyDrive/xray_project/Test_PNG'\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_file_path,\n",
        "    target_size=(512, 512),\n",
        "    color_mode='rgb',\n",
        "    class_mode=None,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "predictions = model.predict(test_generator)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:22:26.705628Z",
          "iopub.execute_input": "2024-09-06T03:22:26.706295Z",
          "iopub.status.idle": "2024-09-06T03:23:20.914399Z",
          "shell.execute_reply.started": "2024-09-06T03:22:26.706256Z",
          "shell.execute_reply": "2024-09-06T03:23:20.913617Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXPLZmqRvYug",
        "outputId": "ff32f9de-72ae-446b-d8aa-cc755b0f844a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 1 classes.\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 771ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# could play around with this and see if it makes much difference in score\n",
        "y_pred = np.where(predictions>0.5, 1, 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:32:12.505664Z",
          "iopub.execute_input": "2024-09-06T03:32:12.506121Z",
          "iopub.status.idle": "2024-09-06T03:32:12.814437Z",
          "shell.execute_reply.started": "2024-09-06T03:32:12.506053Z",
          "shell.execute_reply": "2024-09-06T03:32:12.813024Z"
        },
        "trusted": true,
        "id": "hU9Ei4BYvYug"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count0 = 0\n",
        "count1 = 0\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] == 0:\n",
        "        count0 +=1\n",
        "    else:\n",
        "        count1 += 1\n",
        "print(count0 / len(y_pred))\n",
        "print(count1 / len(y_pred))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:24:03.634248Z",
          "iopub.execute_input": "2024-09-06T03:24:03.634631Z",
          "iopub.status.idle": "2024-09-06T03:24:03.645903Z",
          "shell.execute_reply.started": "2024-09-06T03:24:03.634592Z",
          "shell.execute_reply": "2024-09-06T03:24:03.645074Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tSvWgxAvYug",
        "outputId": "7a9b2ec6-d04d-4a6d-e02d-6e61b36da6c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.717\n",
            "0.283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "y_submission = pd.read_csv('/content/drive/MyDrive/xray_project/data (4).csv')\n",
        "y_submission = y_submission[-2000:].reset_index(drop=True)\n",
        "y_submission.loc[0:1999, 'Finding'] = y_pred\n",
        "y_submission['id'] = y_submission['id'].apply(lambda x: str(x).zfill(5))\n",
        "y_submission.rename(columns={'Finding': 'Outcome'}, inplace=True)\n",
        "y_submission['Outcome'] = y_submission['Outcome'].astype(int)\n",
        "y_submission\n",
        "\n",
        "y_submission.to_csv(f\"submit_{datetime.today().strftime('%Y-%m-%d_%H%M%S')}.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:25:46.246743Z",
          "iopub.execute_input": "2024-09-06T03:25:46.247134Z",
          "iopub.status.idle": "2024-09-06T03:25:46.267641Z",
          "shell.execute_reply.started": "2024-09-06T03:25:46.247096Z",
          "shell.execute_reply": "2024-09-06T03:25:46.266876Z"
        },
        "trusted": true,
        "id": "ij6nv7GWvYuh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "y_submission.to_csv('submission.csv', index=False)\n",
        "with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
        "    zipf.write('submission.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-06T03:24:55.327346Z",
          "iopub.execute_input": "2024-09-06T03:24:55.328075Z",
          "iopub.status.idle": "2024-09-06T03:24:55.336791Z",
          "shell.execute_reply.started": "2024-09-06T03:24:55.328036Z",
          "shell.execute_reply": "2024-09-06T03:24:55.335862Z"
        },
        "trusted": true,
        "id": "bJFhaFZhvYuh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMiDwuusvYuh",
        "outputId": "6a587f88-b4ce-462c-bf26-910599ae3ba2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.2111501e-01],\n",
              "       [2.7750811e-02],\n",
              "       [7.3657058e-02],\n",
              "       [4.1312879e-04],\n",
              "       [5.2581882e-01],\n",
              "       [4.2738961e-03],\n",
              "       [7.4825865e-01],\n",
              "       [9.9385661e-01],\n",
              "       [7.9838810e-03],\n",
              "       [5.6037810e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_eqB5pTBOxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}